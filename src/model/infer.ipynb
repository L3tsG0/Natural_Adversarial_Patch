{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/go/Code/Natural_AE/src/model',\n",
       " '/usr/lib/python311.zip',\n",
       " '/usr/lib/python3.11',\n",
       " '/usr/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/home/go/.cache/pypoetry/virtualenvs/natural-adversarial-patch-Pv5n5N3d-py3.11/lib/python3.11/site-packages',\n",
       " '~/Code/Natural_AE/src']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('~/Code/Natural_AE/src')\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(simpleCNN, self).__init__()\n",
    "        self.name = \"simpleCNN\"\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=16, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# モデルのインスタンスを作成\n",
    "model = simpleCNN(58)\n",
    "\n",
    "# .pthファイルから重みをロード\n",
    "weights = torch.load('trained_CNN.pth',torch.device('cpu'))\n",
    "\n",
    "# モデルに重みを適用\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir: Path, is_train: bool = True, size: int = 224):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (Path): データセットのディレクトリ\n",
    "                訓練 例 Path(\"data/traffic_sign/traffic_Data/DATA\")\n",
    "                テスト 例 Path(\"data/traffic_sign/traffic_Data/TEST\")\n",
    "            is_train (bool, optional): フラグ。 ラベルを得る時に使う。\n",
    "            size (int, optional): 画像のサイズ。デフォルトは224。\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.is_train = is_train\n",
    "        self.size = size\n",
    "        # self.unknown_class = [\"40\", \"41\", \"42\", \"45\", \"49\", \"52\", \"56\", \"57\"]\n",
    "        self.unknown_class = []\n",
    "        self.img_path_list = []\n",
    "        self.num_classes = 58 - len(self.unknown_class)\n",
    "        self.int2label = self.get_int2label()\n",
    "\n",
    "        # 訓練の場合は data_dir/クラス名/ファイル名.png の形式で画像が保存されている\n",
    "        # unknown_class は除外して、画像のパスを取得する\n",
    "        if self.is_train:\n",
    "            for child in self.data_dir.iterdir():\n",
    "                if (\n",
    "                    child.is_dir() and child.name not in self.unknown_class\n",
    "                ):  # unknown_class は除外\n",
    "                    temp_path_list = [\n",
    "                        p for p in child.iterdir() if p.suffix == \".png\"\n",
    "                    ]\n",
    "                    self.img_path_list.extend(temp_path_list)\n",
    "        # テストの場合は data_dir/ファイル名.png の形式で画像が保存されている\n",
    "        else:\n",
    "            for child in self.data_dir.iterdir():\n",
    "                # unknown_class は除外\n",
    "                if (\n",
    "                    child.suffix == \".png\"\n",
    "                    and str(int(child.name[:3])) not in self.unknown_class\n",
    "                ):\n",
    "                    self.img_path_list.append(child)\n",
    "\n",
    "        assert len(self.img_path_list) > 0, \"画像が読み込めませんでした。パスを確認してください。\"\n",
    "\n",
    "        # ラベルの数を取得する。\n",
    "\n",
    "    def custom_transform(self, img):\n",
    "        \"\"\"画像の前処理\"\"\"\n",
    "        img = img.resize(\n",
    "            (self.size, self.size)\n",
    "        )  # 参照: https://www.kaggle.com/code/boulahchichenadir/cnn-classification\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "        )(img)\n",
    "        return img\n",
    "\n",
    "    def get_int2label(self):\n",
    "        int2label = {}\n",
    "        with open(Path(\"labels.csv\")) as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            next(csv_reader)\n",
    "\n",
    "            for row in csv_reader:\n",
    "                key = int(row[0])\n",
    "                value = row[1]\n",
    "                int2label[key] = value\n",
    "\n",
    "        return int2label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_path_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        if self.is_train:\n",
    "            label = int(img_path.parent.name)\n",
    "\n",
    "        else:\n",
    "            # ラベル_ファイル名.png 3桁詰めなので整数に直す\n",
    "            label = int(img_path.name.split(\"_\")[0])\n",
    "            label = int(label)\n",
    "\n",
    "        # 最後に処理\n",
    "        img = self.custom_transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    Path(\"../traffic_Data/TEST\"), is_train=False\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model = simpleCNN(test_dataset.num_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = 0 \n",
    "ok = 0\n",
    "for images, lbls in test_loader:\n",
    "    all +=1\n",
    "    # ここでモデルの学習や推論を行うことができます。\n",
    "    output = model(images)\n",
    "    # print(torch.argmax(output))\n",
    "    # print(lbls)\n",
    "    if torch.argmax(output)==lbls:\n",
    "        ok+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01354062186559679\n"
     ]
    }
   ],
   "source": [
    "print(ok/all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (Image, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m images \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(Path(\u001b[39m\"\u001b[39m\u001b[39m../traffic_Data/DATA/0/000_0001.png\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mresize((\u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m output \u001b[39m=\u001b[39m model(images)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/natural-adversarial-patch-Pv5n5N3d-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36msimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39m56\u001b[39m \u001b[39m*\u001b[39m \u001b[39m56\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/natural-adversarial-patch-Pv5n5N3d-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/natural-adversarial-patch-Pv5n5N3d-py3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/natural-adversarial-patch-Pv5n5N3d-py3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (Image, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "# images = Image.open(Path(\"../traffic_Data/DATA/0/000_0001.png\"))\n",
    "# images = images.resize((256,256))\n",
    "output = model(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural-adversarial-patch-Pv5n5N3d-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
